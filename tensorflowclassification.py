# -*- coding: utf-8 -*-
"""TensorFlowClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GEe2EWOj1qJU9kddVacjvUmIzzOuTWz
"""

!pip install tensorflow-gpu

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount("/content/drive")

dir = '/content/drive/MyDrive/UdemyRayan/'

# Rating data for sentiment analysis  of amazon alexa (positive or negative feedback (1 or 0))
txt_df = pd.read_csv(dir + "Data/amazon_alexa.tsv", sep = "\t")
txt_df.head()

# visualize data
txt_df.shape

txt_df.info()

txt_df.describe()

txt_df["verified_reviews"]

"""Data Visualize"""

positive = txt_df[txt_df["feedback"]==1]
negative = txt_df[txt_df["feedback"]==0]

negative

"""It is Unbalanced data"""

sns.countplot(txt_df["feedback"], label = "Count")

sns.countplot(txt_df["rating"])

sns.histplot(x = "rating", data= txt_df, bins = 5)

txt_df["rating"].hist(bins = 5)

# check the correlation
plt.figure(figsize=(15,5))
sns.barplot(x = "variation", y = "rating", data = txt_df, palette= 'deep')
plt.xticks(rotation = 90)

"""Clean Up the data"""

txt_df.columns

txt_df.drop(['rating', 'date'], axis = 1, inplace= True)

txt_df.head()

"""dummy variatin for variation"""

variation_dummies = pd.get_dummies(txt_df['variation'], drop_first= True)

variation_dummies

txt_df.drop("variation", axis= 1, inplace= True)

txt_df = pd.concat([variation_dummies, txt_df], axis= 1)

txt_df

"""Count Vectorizer example!"""

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ["My name is Rudra.", "My father is Toparam.", "How funny are you!", "Is this Rabbit hole?"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)

X.toarray()

## looking at the unique data
vectorizer.get_feature_names()

"""Apply count vectorizer to the amazon data"""

vectorizer = CountVectorizer()
txt_vectorizer =vectorizer.fit_transform(txt_df['verified_reviews'])

txt_vectorizer.toarray()

vectorizer.get_feature_names()

txt_vectorizer.shape

txt_df.drop("verified_reviews", axis= 1, inplace= True)

# convert tocken to data frame
reviews_df = pd.DataFrame(txt_vectorizer.toarray())
reviews_df

# Concatenate
txt_df = pd.concat([txt_df,reviews_df], axis = 1)
txt_df.head()

# X and y label
y = txt_df["feedback"].values
X = txt_df.drop("feedback", axis =1).values

X.shape

## test train split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)

X_train.shape

y_train.shape

y_train

y_test.shape

X_test.shape

"""Building ANN Classfifies"""

Ann_classifier = tf.keras.models.Sequential()
Ann_classifier.add(tf.keras.layers.Dense(units= 400, activation= 'relu', input_shape = (4059,)))
Ann_classifier.add(tf.keras.layers.Dense(units = 400, activation= 'relu'))
Ann_classifier.add(tf.keras.layers.Dense(units =1, activation= 'sigmoid'))

Ann_classifier.summary()

# compile
Ann_classifier.compile(optimizer= 'Adam', loss= 'binary_crossentropy', metrics= 'Accuracy')

#@title
# fit the model
epoch_hist = Ann_classifier.fit(X_train, y_train, epochs= 200, batch_size= 50, validation_split= 0.2)

epoch_hist.history.keys()

plt.plot(epoch_hist.history['accuracy'])
plt.plot(epoch_hist.history['val_accuracy'])
plt.xlabel("Epochs")
plt.ylabel("Training Loss")

y_pred_test = Ann_classifier.predict(X_test)

y_pred_test = y_pred_test >0.5

# confusion matrix
from sklearn.metrics import  confusion_matrix 
cm = confusion_matrix(y_test, y_pred_test)

sns.heatmap(cm, annot= True)

cm

"""Build next model to preditc Diabetes patients"""

diabetes_df = pd.read_csv(dir + "Data/diabetes.csv")
diabetes_df.head()

diabetes_df.info()

diabetes_df.describe()

sns.pairplot(diabetes_df, hue= 'Outcome')

sns.countplot(diabetes_df["Outcome"])

sns.heatmap(diabetes_df.corr(), annot= True)

X = diabetes_df.drop("Outcome", axis= 1)
y = diabetes_df["Outcome"]

# feature scaling (must for ANN)
from sklearn.preprocessing import  StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

## train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test  = train_test_split(X_scaled, y, test_size = 0.2)

X_train.shape
X_test.shape

X_train.shape

## Build classification model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units= 400, activation= 'relu', input_shape = (8,)))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(units=400, activation= 'relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

model.summary()

# compile model
model.compile(optimizer= 'Adam', loss = "binary_crossentropy", metrics= "accuracy")

# fit model 
epoch_hist = model.fit(X_train, y_train, epochs = 500)

epoch_hist.history.keys()

plt.plot(epoch_hist.history['loss'])
#plt.plot(epoch_hist.history['val_loss'])
plt.xlabel("Epochs")
plt.ylabel("Training Loss")



plt.plot(epoch_hist.history['accuracy'])
#plt.plot(epoch_hist.history['val_accuracy'])
plt.xlabel("Accuracy")
plt.ylabel("Epochs")

y_pred = model.predict(X_test)
y_pred = y_pred > 0.5

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot= True)

cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm, annot= True)

cm

## report
from sklearn.metrics import  classification_report
print(classification_report(y_test, y_pred))

